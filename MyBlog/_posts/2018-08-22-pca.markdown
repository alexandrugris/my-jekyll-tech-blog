---
layout: post
title:  "Principal Component Analisys"
date:   2018-08-15 13:15:16 +0200
categories: statistics
---
This is a continuation on my previous post on linear regression. It talks about finding a set of axes (dimensions) along which to regress in such a way as to preserve the most relevant data while avoiding multicollinearity and reducing the amount of dimensions.

### PCA

Mathematical optimization problem to find the direction line along which the distance between projected data points is the greatest, that is it has the largest variance. We choose this direction because the largest amount of information about the value of each point is preserved in this projection. The second principal component is an axis perpendicular to the first axis and so on, for multi-dimensional coordiante spaces. This choice of axes not only preserves the most amount of information, but also avoids multicollinearity, as the principal compoments are perpendicular to each other.

The problem can be rephrased as: given a set of highly correlated factors, `[X1 ... Xn]`, find a equal number of factors, `[F1 ... Fn]`, completely uncorrelated to each other, sorted so that `Var(F1) > ... > Var(Fn)`, on which we are able to describe our regression problem as `Y = A + B1 * F1 + ... Bn * Fn`. These factors satisfy the equality `Var(F) == Var(X)`, meaning we don't lose information when we do the decomposition along these new axes. 

*It is important to notice the inequality, ``Var(F1) > ... > Var(Fn)`. The higher the variance along that particular axis, the more information is contained in that axis.*.  

The linear algebra problem that helps us find these factors is called *eigenvalue decomposition* and can be phrased as:

> Find `F1 = a1X1 + ... + anXn` such that `Var(F1)` is maximised and the coefficients `a1 .. an` satisfy the contraint `a1^2 + ... + an^2 = 1`. `F1` is the principal component 1, `v1 = [a1,...,an]` is called *eigenvector 1* and `e1 = Var(F1)` is called the *eigenvalue* of the principal component `F1`.

> Principal component `F2 = b1(X1 - F1) + ... + bn(Xn - F1)` is subject to the same constraints. Like this, we have defined a recurrent problem which allows us to find all `Fn` components.

Results from decomposition are:

- eigenvalues: tell us how much of the variance can be explain by this particular component

- the principal components themselves: these can be used in regression if the eigenvalues are high enough

- eigenvectors: which are needed to calculate the principal components as follows:
`[Fi] = [Xi] * [Vi]`. That is, the column matrix containing all factors is the matrix product between the column matrix of eigenvectors `Vi=[a1 ... an]` and the original factor matrix, `X=[X1 ... Xn]`, (n factors, each with k elements)

Now, dividing each eigenvalue `ei=Var(Fi)` to the total variance of F, `Var(F) = Var(F1) + ... Var(Fn)` since `Covar(Fi, Fj) = 0`, we obtain a vector `v` with `sum(vi) = 1`. The plot of this vector is called *scree plot* and shows how much each factor contributes to explaining the variance of the original data. This is the most important decision tool for us to decide which factors we keep in our analysis.

More information [here](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix)

### Eigenvalue decomposition

The following Python code uses the power method together with deflation for computing the eigenvectors and eigenvalues for a *square* matrix `A`.

```python
def vector_norm(v):
    return np.sqrt(np.dot(v, v))

def dominant_eigenvector(A):
    
    """ Returns the principal eigen vector and its corresponding eigen value. uses the power method technique """
    
    # check that A is square
    assert(A.shape[0] == A.shape[1])
    
    v = np.random.rand(A.shape[1])
    v_prim = np.zeros(A.shape[1])
    
    lmbda = 0
    
    # TODO: add raleigh for faster convergence
    
    while np.max(v-v_prim) > 1e-10:
       v_prim = np.dot(A, v)
       lmbda = vector_norm(v_prim)
       v, v_prim = (v_prim / lmbda, v)
       
    # eigenvalue, normalized eigenvector
    return (lmbda, v)

def eigen_decomposition(A):
    
    # check that A is square
    assert(A.shape[0] == A.shape[1])
    
    vectors = []
    lambdas = []
    
    for i in range(0, A.shape[1]):
        
        lmbda, v = eigen_vector(A)
        
        vectors.append(v),
        lambdas.append(lmbda)
        
        # power method deflation eigenvectors
        # idea is to remove the initial contribution from the initial space
        A = A - lmbda * np.matmul(v.reshape(A.shape[1], 1), v.reshape(1, A.shape[1]))
        
        # each vector is perpendicular to the rest
       
    
    return (lambdas, vectors)
```

Correctness is very simple to check. Do the following:

1. Check that `A * lambda == lambda * v` - condition for eigenvector
2. Check that all vectors are perpendicular to eachother: `np.dot(v1, v2) == 0` - condition for all the eigenvectors